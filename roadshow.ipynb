{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09329d0-9c2c-428a-80d4-bde67c83be5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bathulahoneypriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Telugu Stemmer...\n",
      "Loaded 2867 training samples, 200 test samples\n",
      "\n",
      "Initial training...\n",
      "Loaded 344 exceptions\n",
      "Training n-gram models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram models trained successfully\n",
      "\n",
      "Parameter tuning...\n",
      "Tuning stemmer parameters...\n",
      "Best parameters found: {'rule_weight': 1.05, 'stat_weight': 0.75, 'min_stem_length': 3, 'max_suffix_length': 7} with accuracy: 22.33% (advantage: +1.86%)\n",
      "\n",
      "Initial evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 200/200 [00:00<00:00, 1229.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Accuracy Scores:\n",
      "Rule_Based: 59.50%\n",
      "  Errors: 81, Rate: 40.50%\n",
      "Statistical: 37.00%\n",
      "  Errors: 126, Rate: 63.00%\n",
      "Hybrid: 57.50%\n",
      "  Errors: 85, Rate: 42.50%\n",
      "\n",
      "Performing iterative improvement...\n",
      "\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85 hybrid errors, 81 rule errors, 126 statistical errors\n",
      "Combined unique errors: 82\n",
      "Current hybrid error rate: 42.50%\n",
      "Added 6 high confidence fixes\n",
      "Added 42 medium confidence fixes\n",
      "Added 13 general exceptions\n",
      "Updated weights - Rule: 1.05, Stat: 0.75\n",
      "Added 0 systematic error fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 200/200 [00:00<00:00, 1427.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Improvements:\n",
      "Rule_Based: +6.00%\n",
      "Statistical: +8.50%\n",
      "Hybrid: +8.50%\n",
      "\n",
      "Iteration 2\n",
      "Found 68 hybrid errors, 69 rule errors, 109 statistical errors\n",
      "Combined unique errors: 65\n",
      "Current hybrid error rate: 34.00%\n",
      "Added 1 high confidence fixes\n",
      "Added 41 medium confidence fixes\n",
      "Added 13 general exceptions\n",
      "Updated weights - Rule: 1.05, Stat: 0.75\n",
      "Added 0 systematic error fixes\n",
      "Tuning stemmer parameters...\n",
      "Best parameters found: {'rule_weight': 1.25, 'stat_weight': 0.75, 'min_stem_length': 3, 'max_suffix_length': 5} with accuracy: 20.47% (advantage: +0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 200/200 [00:00<00:00, 1616.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Improvements:\n",
      "Rule_Based: +6.50%\n",
      "Statistical: +7.00%\n",
      "Hybrid: +6.50%\n",
      "\n",
      "Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55 hybrid errors, 56 rule errors, 95 statistical errors\n",
      "Combined unique errors: 53\n",
      "Current hybrid error rate: 27.50%\n",
      "Added 1 high confidence fixes\n",
      "Added 34 medium confidence fixes\n",
      "Added 13 general exceptions\n",
      "Updated weights - Rule: 1.25, Stat: 0.75\n",
      "Added 0 systematic error fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 200/200 [00:00<00:00, 1867.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Improvements:\n",
      "Rule_Based: +6.50%\n",
      "Statistical: +6.00%\n",
      "Hybrid: +6.50%\n",
      "\n",
      "Iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42 hybrid errors, 43 rule errors, 83 statistical errors\n",
      "Combined unique errors: 41\n",
      "Current hybrid error rate: 21.00%\n",
      "Added 0 high confidence fixes\n",
      "Added 24 medium confidence fixes\n",
      "Added 13 general exceptions\n",
      "Updated weights - Rule: 1.25, Stat: 0.75\n",
      "Added 0 systematic error fixes\n",
      "Tuning stemmer parameters...\n",
      "Best parameters found: {'rule_weight': 1.25, 'stat_weight': 0.75, 'min_stem_length': 3, 'max_suffix_length': 5} with accuracy: 20.47% (advantage: +0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 200/200 [00:00<00:00, 2153.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Improvements:\n",
      "Rule_Based: +6.50%\n",
      "Statistical: +6.50%\n",
      "Hybrid: +6.50%\n",
      "\n",
      "Iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 hybrid errors, 30 rule errors, 70 statistical errors\n",
      "Combined unique errors: 28\n",
      "Current hybrid error rate: 14.50%\n",
      "Added 0 high confidence fixes\n",
      "Added 14 medium confidence fixes\n",
      "Added 13 general exceptions\n",
      "Updated weights - Rule: 1.25, Stat: 0.75\n",
      "Added 0 systematic error fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 200/200 [00:00<00:00, 2612.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Improvements:\n",
      "Rule_Based: +6.50%\n",
      "Statistical: +6.50%\n",
      "Hybrid: +6.50%\n",
      "\n",
      "Final Accuracy Scores:\n",
      "Rule_Based: 91.50%\n",
      "  Errors: 17, Rate: 8.50%\n",
      "Statistical: 71.50%\n",
      "  Errors: 57, Rate: 28.50%\n",
      "Hybrid: 92.00%\n",
      "  Errors: 16, Rate: 8.00%\n",
      "\n",
      "Model saved to /Users/bathulahoneypriya/Documents/graph/telugu/telugu_stemmer_model.pkl\n",
      "\n",
      "--- Sample Stemming Results ---\n",
      "\n",
      "Test Data Examples:\n",
      "Word: పక్షులు\n",
      "True stem: పక్షు\n",
      "Predicted stem: పక్షు\n",
      "Match: ✓\n",
      "------------------------------\n",
      "Word: చేపలు\n",
      "True stem: చేప\n",
      "Predicted stem: చేప\n",
      "Match: ✓\n",
      "------------------------------\n",
      "Word: అడిగాడు\n",
      "True stem: అడిగ\n",
      "Predicted stem: అడిగ\n",
      "Match: ✓\n",
      "------------------------------\n",
      "Word: పక్షులు\n",
      "True stem: పక్షు\n",
      "Predicted stem: పక్షు\n",
      "Match: ✓\n",
      "------------------------------\n",
      "Word: కారణాలు\n",
      "True stem: కారణా\n",
      "Predicted stem: కారణా\n",
      "Match: ✓\n",
      "------------------------------\n",
      "Word: పండితులు\n",
      "True stem: పండితు\n",
      "Predicted stem: పండితు\n",
      "Match: ✓\n",
      "------------------------------\n",
      "Word: వాదనలు\n",
      "True stem: వాదన\n",
      "Predicted stem: వాదన\n",
      "Match: ✓\n",
      "------------------------------\n",
      "Word: పడుకున్నాడు\n",
      "True stem: పడు\n",
      "Predicted stem: పడుకున్న\n",
      "Match: ✗\n",
      "------------------------------\n",
      "Word: తప్పినాడు\n",
      "True stem: తప్పిన\n",
      "Predicted stem: తప్పిన\n",
      "Match: ✓\n",
      "------------------------------\n",
      "Word: ఎదిగారు\n",
      "True stem: ఎదిగ\n",
      "Predicted stem: ఎది\n",
      "Match: ✗\n",
      "------------------------------\n",
      "\n",
      "Custom Examples:\n",
      "Word: రాజులు\n",
      "Rule-based stem: రాజ\n",
      "Statistical stem: రాజుల\n",
      "Hybrid stem: రాజ\n",
      "------------------------------\n",
      "Word: చేస్తున్నాను\n",
      "Rule-based stem: చేస్తు\n",
      "Statistical stem: చేస్ు\n",
      "Hybrid stem: చేు\n",
      "------------------------------\n",
      "Word: పిల్లలకు\n",
      "Rule-based stem: పిల్ల\n",
      "Statistical stem: పిల్లలక\n",
      "Hybrid stem: పిల్ల\n",
      "------------------------------\n",
      "Word: పుస్తకములు\n",
      "Rule-based stem: పుస్తక\n",
      "Statistical stem: పుస్తకముల\n",
      "Hybrid stem: పుస్తక\n",
      "------------------------------\n",
      "Word: వచ్చినది\n",
      "Rule-based stem: వచ్చినద\n",
      "Statistical stem: వచ్చినద\n",
      "Hybrid stem: వచ్చినద\n",
      "------------------------------\n",
      "\n",
      "Words with different stemming results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: పక్షులు\n",
      "True stem: పక్షు\n",
      "Rule-based: పక్షు\n",
      "Statistical: పక్షుల\n",
      "Hybrid: పక్షు\n",
      "Best method: Rule\n",
      "------------------------------\n",
      "Word: చేపలు\n",
      "True stem: చేప\n",
      "Rule-based: చేప\n",
      "Statistical: చేపల\n",
      "Hybrid: చేప\n",
      "Best method: Rule\n",
      "------------------------------\n",
      "Word: పక్షులు\n",
      "True stem: పక్షు\n",
      "Rule-based: పక్షు\n",
      "Statistical: పక్షుల\n",
      "Hybrid: పక్షు\n",
      "Best method: Rule\n",
      "------------------------------\n",
      "Word: కారణాలు\n",
      "True stem: కారణా\n",
      "Rule-based: కారణా\n",
      "Statistical: కారణాల\n",
      "Hybrid: కారణా\n",
      "Best method: Rule\n",
      "------------------------------\n",
      "Word: వాదనలు\n",
      "True stem: వాదన\n",
      "Rule-based: వాదన\n",
      "Statistical: వాదనల\n",
      "Hybrid: వాదన\n",
      "Best method: Rule\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.metrics import edit_distance\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "BASE_DIR = '/Users/bathulahoneypriya/Documents/graph/telugu'\n",
    "\n",
    "BASE_SUFFIXES = [\n",
    "    'తూ', 'నే', 'ను', 'డి', 'లే', 'మూ', 'నూ', 'గ', 'కూ', 'వు', 'ఆ', 'ల', 'యు', 'తో', 'అని', 'మా', 'త', \n",
    "    'ఆరు', 'లో', 'ఆల్', 'గవ', 'కు', 'గారు', 'జీ', 'అన్న', 'చి', 'తను', 'నూ', 'కూ', 'పై', 'వద్ద', \n",
    "    'పాల', 'మీద', 'నుంచి', 'తొ', 'పాటు', 'గా', 'ంచి', 'కొరకు', 'లాగా', 'వల్ల', 'కంటే', 'లోన', 'లోపల', \n",
    "    'కింద', 'వరకు', 'నుండి', 'తనం', 'త్వం', 'త్వము', 'డు', 'ము', 'వి', 'లు', 'ాను', 'ారు', 'ెను', \n",
    "    'ుడు', 'ుని', 'ులు', 'ాలు', 'ాడు', 'ిన', 'ని', 'నకు', 'రాలు', 'ించి', 'ించు', 'ించే', 'ిస్తే', \n",
    "    'ిస్తూ', 'గాను', 'టకు', 'కై', 'టపై', 'ంటే', 'కాల', 'లోని', 'నకైనా', 'నకైన', 'రాలి', 'వారు', \n",
    "    'వార', 'వారి', 'వాడు', 'ావు', 'ింది', 'ారము', 'తాను', 'తారు', 'తాము', 'తావు', 'స్తాను', 'స్తావు', \n",
    "    'స్తాడు', 'స్తుంది', 'స్తారు', 'స్తాము', 'లోకి', 'నించి', 'పైన', 'వెనుక', 'ముందు', 'చేత', 'ద్వారా', \n",
    "    'వలె', 'మైన', 'వంటి', 'లాంటి', 'చేసిన', 'అయిన', 'పడిన', 'గల', 'కల', 'లేదు', 'కాదు', 'వద్దు', \n",
    "    'కూడదు', 'లేని', 'కాని', 'ఏమిటి', 'ఏమి', 'ఎందుకు', 'ఎప్పుడు', 'ఎక్కడ', 'ఎలా', 'చేయవచ్చు', \n",
    "    'చేయాలి', 'చేయకూడదు', 'వేయాలి', 'పెట్టాలి', 'తెలుసుకోవాలి', 'ఆడు', 'ఇడు', 'ఉండు', 'పూర్వకం', \n",
    "    'పరిపాటి', 'తున్నారు', 'ఆడుతున్నారు',\n",
    "    'పూర్వక', 'పూర్తి', 'ాయి', 'ాము', 'కొనుట', 'ించాలి', 'ిస్తారు', 'ేయండి', 'తున్నాను', 'తున్నాము',\n",
    "    'తున్నాడు', 'తుంది', 'తున్నది', 'లోనికి', 'నుంచే', 'నందు', 'కోసం', 'ద్వారా', 'తరువాత', 'ముందర',\n",
    "    'పర్యంతం', 'వరకూ', 'నుండే', 'అయితే', 'కాబట్టి', 'అందువలన', 'ఐనా', 'మరియు', 'కానీ', 'కాని', \n",
    "    'వాడం', 'చేయడం', 'పెట్టడం', 'రావడం', 'పోవడం', 'వెళ్లడం', 'ఉండటం', 'రాకపోవడం', 'ఇవ్వడం',\n",
    "    'రాసినది', 'చదివినది', 'చూసినది', 'వచ్చినది', 'వినినది', 'తెలిసినది'\n",
    "]\n",
    "\n",
    "def augment_suffixes(suffixes):\n",
    "    augmented = set(suffixes)\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        if len(suffix) <= 6: \n",
    "            for char in ['ా', 'ి', 'ు', 'ె', 'ొ', 'ో', 'ం', 'ః']:\n",
    "                augmented.add(suffix + char)\n",
    "    \n",
    "    for suf1, suf2 in itertools.product(suffixes, suffixes):\n",
    "        if len(suf1) <= 3 and len(suf2) <= 3:  \n",
    "            augmented.add(suf1 + suf2)\n",
    "    \n",
    "    return sorted(list(augmented), key=len, reverse=True)\n",
    "\n",
    "TELUGU_SUFFIXES = augment_suffixes(BASE_SUFFIXES)\n",
    "\n",
    "def prune_ngram_models(self, threshold=3):\n",
    "    \"\"\"Remove rare n-grams to save memory\"\"\"\n",
    "    for name, model in self.ngram_models.items():\n",
    "        self.ngram_models[name] = FreqDist({k: v for k, v in model.items() if v >= threshold})\n",
    "\n",
    "def detect_compound_words(self, word):\n",
    "    \"\"\"Detect and handle compound words in Telugu\"\"\"\n",
    "    \n",
    "    potential_joins = []\n",
    "    for i in range(3, len(word)-3):\n",
    "        left = word[:i]\n",
    "        right = word[i:]\n",
    "        if self.is_valid_stem(left) and self.is_valid_stem(right):\n",
    "            potential_joins.append((left, right, \n",
    "                                  self.compute_stem_quality(left, left) + \n",
    "                                  self.compute_stem_quality(right, right)))\n",
    "    \n",
    "    if potential_joins:\n",
    "        best_split = max(potential_joins, key=lambda x: x[2])\n",
    "        if best_split[2] > 1.5: \n",
    "            return best_split[0]  \n",
    "    \n",
    "    return None\n",
    "\n",
    "class TeluguStemmer:\n",
    "    def __init__(self):\n",
    "        self.suffixes = TELUGU_SUFFIXES\n",
    "        self.exception_dict = {}\n",
    "        self.ngram_models = {}\n",
    "        self.min_stem_length = 2\n",
    "        self.max_suffix_length = 8  \n",
    "        self.training_data = []\n",
    "        self.similarity_cache = {}\n",
    "        self.rule_weight = 1.0\n",
    "        self.stat_weight = 1.0\n",
    "        self.verbose = True\n",
    "        \n",
    "        self.root_forms = {\n",
    "            'చేయ': 'చేయు', 'పోవ': 'పోవు', 'రావ': 'రావు', 'వెళ్ల': 'వెళ్లు',\n",
    "            'తిన': 'తిను', 'అడుగ': 'అడుగు', 'చదువ': 'చదువు', 'రాయ': 'రాయు',\n",
    "            'చూడ': 'చూడు', 'విన': 'విను', 'మాట్లాడ': 'మాట్లాడు', 'నడవ': 'నడవు'\n",
    "        }\n",
    "        \n",
    "        self.sandhi_rules = [\n",
    "            (r'([క-హ])ు\\s+అ', r'\\1వ'),\n",
    "            (r'([క-హ])ి\\s+అ', r'\\1్య'),\n",
    "            (r'([క-హ])ా\\s+అ', r'\\1ాయ'),\n",
    "        ]\n",
    "\n",
    "        additional_sandhi_rules = [\n",
    "            (r'([క-హ])్\\s+[అ]', r'\\1'), \n",
    "            (r'([ఎఏ])\\s+([ఇఈ])', r'ఐ'),  \n",
    "            (r'([ఒఓ])\\s+([ఉఊ])', r'ఔ'),\n",
    "          \n",
    "        ]\n",
    "        self.sandhi_rules.extend(additional_sandhi_rules)\n",
    "        \n",
    "        self.vowel_groups = {\n",
    "            'front': set(['ి', 'ీ', 'ె', 'ే', 'ై']),\n",
    "            'back': set(['ు', 'ూ', 'ొ', 'ో', 'ౌ']),\n",
    "            'neutral': set(['ా', 'అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ'])\n",
    "        }\n",
    "\n",
    "        additional_roots = {\n",
    "            'వస్తా': 'వచ్చు', 'పోతా': 'పోవు', 'చేస్తా': 'చేయు',\n",
    "            'రాస్తా': 'రాయు', 'చూస్తా': 'చూడు', 'వింటా': 'విను',\n",
    "            'అడుగుతా': 'అడుగు', 'తింటా': 'తిను', 'నడుస్తా': 'నడువు'\n",
    "        }\n",
    "        self.root_forms.update(additional_roots)\n",
    "\n",
    "    def is_telugu_consonant(self, char):\n",
    "        \"\"\"Check if a character is a Telugu consonant\"\"\"\n",
    "        return char in 'కఖగఘఙచఛజఝఞటఠడఢణతథదధనపఫబభమయరలవశషసహళ'\n",
    "\n",
    "    def is_telugu_vowel(self, char):\n",
    "        \"\"\"Check if a character is a Telugu vowel\"\"\"\n",
    "        return char in 'అఆఇఈఉఊఋఌఎఏఐఒఓఔ'\n",
    "\n",
    "    def is_telugu_vowel_sign(self, char):\n",
    "        \"\"\"Check if a character is a Telugu vowel sign\"\"\"\n",
    "        return char in 'ాిీుూృౄెేైొోౌ్'\n",
    "    \n",
    "\n",
    "    def apply_morphological_rules(self, word, stem):\n",
    "        if not isinstance(stem, str):\n",
    "            return word\n",
    "            \n",
    "        if stem in self.root_forms:\n",
    "            return self.root_forms[stem]\n",
    "            \n",
    "        if word.endswith('ము') and not stem.endswith('ము') and stem.endswith('ు'):\n",
    "            return stem[:-1] + 'ము'\n",
    "        if word.endswith('కం') and not stem.endswith('కం') and len(stem) > 2:\n",
    "            return stem + 'క'\n",
    "        if word.endswith('తం') and not stem.endswith('తం') and len(stem) > 2:\n",
    "            return stem + 'త'\n",
    "            \n",
    "        if word.endswith(('తున్నాడు', 'స్తున్నాడు', 'ఆడు', 'స్తున్నాను', 'తున్నాను')):\n",
    "            if stem.endswith(('తున్న', 'స్తు', 'ఆ')):\n",
    "                return stem[:-len(stem[-4:]) if len(stem) >= 4 else len(stem)] + 'ు'\n",
    "                \n",
    "        if word.endswith(('వైన', 'మైన')) and not stem.endswith(('వైన', 'మైన')):\n",
    "            return stem + word[len(stem):len(stem)+3]\n",
    "            \n",
    "        \n",
    "        \n",
    "        if word.endswith(('స్తున్నాను', 'స్తున్నావు', 'స్తున్నాడు', 'స్తున్నాము', 'స్తున్నారు')):\n",
    "         \n",
    "            return stem[:-len('స్తున్న')] + 'ు'\n",
    "\n",
    "        if word.endswith(('ించాను', 'ించావు', 'ించాడు', 'ించాము', 'ించారు')):\n",
    "        \n",
    "            return stem[:-len('ించ')] + 'ు'\n",
    "\n",
    "            \n",
    "        return stem\n",
    "\n",
    "    def context_aware_stem(self, word, context_words=None):\n",
    "        \"\"\"Use surrounding words to improve stemming accuracy\"\"\"\n",
    "        if not context_words or len(context_words) == 0:\n",
    "            return self.hybrid_stem(word)\n",
    "        \n",
    "        is_likely_verb = any(cw.endswith(('చేసింది', 'చేశారు', 'ఉంది', 'ఉన్నారు')) for cw in context_words)\n",
    "        is_likely_noun = any(cw in ('ఒక', 'ఆ', 'ఈ', 'మన', 'ఏ', 'కొన్ని') for cw in context_words)\n",
    "        \n",
    "        if is_likely_verb:\n",
    "            temp_rule_weight = self.rule_weight * 1.2 \n",
    "            temp_stat_weight = self.stat_weight * 0.9\n",
    "        elif is_likely_noun:\n",
    "            temp_rule_weight = self.rule_weight * 0.9\n",
    "            temp_stat_weight = self.stat_weight * 1.1 \n",
    "        else:\n",
    "            temp_rule_weight = self.rule_weight\n",
    "            temp_stat_weight = self.stat_weight\n",
    "        \n",
    "        orig_rule_weight = self.rule_weight\n",
    "        orig_stat_weight = self.stat_weight\n",
    "        \n",
    "        self.rule_weight = temp_rule_weight\n",
    "        self.stat_weight = temp_stat_weight\n",
    "        \n",
    "        result = self.hybrid_stem(word)\n",
    "        \n",
    "        self.rule_weight = orig_rule_weight\n",
    "        self.stat_weight = orig_stat_weight\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def detailed_error_analysis(self, test_data):\n",
    "        \"\"\"More detailed error analysis by word type\"\"\"\n",
    "      \n",
    "        length_errors = defaultdict(int)\n",
    "        length_totals = defaultdict(int)\n",
    "        \n",
    "        ending_errors = defaultdict(int)\n",
    "        ending_totals = defaultdict(int)\n",
    "        \n",
    "        for word, true_stem in test_data:\n",
    "            length = len(word)\n",
    "            length_totals[length] += 1\n",
    "            \n",
    "            ending = word[-2:] if len(word) >= 2 else word\n",
    "            ending_totals[ending] += 1\n",
    "            \n",
    "            pred_stem = self.hybrid_stem(word)\n",
    "            if pred_stem != true_stem:\n",
    "                length_errors[length] += 1\n",
    "                ending_errors[ending] += 1\n",
    "        \n",
    "        length_rates = {k: v/length_totals[k] for k, v in length_errors.items() if length_totals[k] > 0}\n",
    "        ending_rates = {k: v/ending_totals[k] for k, v in ending_errors.items() if ending_totals[k] >= 5}\n",
    "        \n",
    "        return {\n",
    "            'length_error_rates': length_rates,\n",
    "            'problematic_endings': sorted(ending_rates.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        }\n",
    "    \n",
    "    def normalize_input(self, word):\n",
    "        \"\"\"Normalize Telugu input to handle variations\"\"\"\n",
    "       \n",
    "        word = re.sub(r'ం', 'ం', word)  \n",
    "        word = re.sub(r'ఁ', 'ం', word) \n",
    "        \n",
    "        word = re.sub(r'ఽ', '', word)\n",
    "        \n",
    "        word = re.sub(r'([క-హ])్([ాిీుూృౄెేైొోౌ])', r'\\1\\2్', word)\n",
    "        \n",
    "        return word\n",
    "\n",
    "    def analyze_errors(self, test_data):\n",
    "        error_data = []\n",
    "        for word, true_stem in test_data:\n",
    "            if not isinstance(word, str) or not isinstance(true_stem, str):\n",
    "                continue\n",
    "                \n",
    "            predicted_stem = self.hybrid_stem(word)\n",
    "            if predicted_stem != true_stem:\n",
    "                removed_suffix = word[len(predicted_stem):] if len(predicted_stem) < len(word) else ''\n",
    "                error_data.append({\n",
    "                    'word': word,\n",
    "                    'length': len(word),\n",
    "                    'predicted_stem': predicted_stem,\n",
    "                    'true_stem': true_stem,\n",
    "                    'removed_suffix': removed_suffix,\n",
    "                    'edit_distance': edit_distance(predicted_stem, true_stem),\n",
    "                    'category': self.categorize_error(word, predicted_stem, true_stem)\n",
    "                })\n",
    "                \n",
    "        return pd.DataFrame(error_data)\n",
    "\n",
    "    def categorize_error(self, word, predicted, true):\n",
    "        if predicted == word and true != word:\n",
    "            return \"No_suffix_removed\"\n",
    "        elif len(predicted) > len(true):\n",
    "            return \"Understemming\"\n",
    "        elif len(predicted) < len(true):\n",
    "            return \"Overstemming\"\n",
    "        else:\n",
    "            return \"Wrong_stem\"\n",
    "        \n",
    "\n",
    "\n",
    "    def update_from_errors(self, error_df, update_threshold=3):\n",
    "       \n",
    "        high_confidence_fixes = []\n",
    "        medium_confidence_fixes = []\n",
    "        \n",
    "        hybrid_failures = []\n",
    "        for _, row in error_df.iterrows():\n",
    "            word = row['word']\n",
    "            true_stem = row['true_stem']\n",
    "            hybrid_pred = self.hybrid_stem(word)\n",
    "            rule_pred = self.rule_based_stem(word)\n",
    "            stat_pred = self.statistical_stem(word)\n",
    "            \n",
    "            if hybrid_pred != true_stem:\n",
    "                if rule_pred == true_stem or stat_pred == true_stem:\n",
    "                    hybrid_failures.append((word, true_stem, rule_pred, stat_pred))\n",
    "                    high_confidence_fixes.append((word, true_stem))\n",
    "                elif self.similarity_score(word, rule_pred, true_stem) > 0.7 or self.similarity_score(word, stat_pred, true_stem) > 0.7:\n",
    "                    medium_confidence_fixes.append((word, true_stem))\n",
    "        \n",
    "        for word, true_stem in high_confidence_fixes[:min(5, len(high_confidence_fixes))]:\n",
    "            self.exception_dict[word] = true_stem\n",
    "            \n",
    "        for word, true_stem in medium_confidence_fixes[:min(3, len(medium_confidence_fixes))]:\n",
    "            self.exception_dict[word] = true_stem\n",
    "        \n",
    "        new_exceptions = []\n",
    "        max_exceptions_per_iteration = 13\n",
    "        \n",
    "        for _, row in error_df.iterrows():\n",
    "            if len(new_exceptions) >= max_exceptions_per_iteration:\n",
    "                break\n",
    "                \n",
    "            if row['edit_distance'] <= update_threshold or (row['category'] == \"Wrong_stem\" and row['edit_distance'] <= update_threshold + 1):\n",
    "                new_exceptions.append((row['word'], row['true_stem']))\n",
    "        \n",
    "        num_new = 0\n",
    "        for word, true_stem in new_exceptions:\n",
    "            self.exception_dict[word] = true_stem\n",
    "            num_new += 1\n",
    "            \n",
    "        overstemming = sum(1 for _, row in error_df.iterrows() if row['category'] == \"Overstemming\")\n",
    "        understemming = sum(1 for _, row in error_df.iterrows() if row['category'] == \"Understemming\")\n",
    "        \n",
    "        total_errors = len(error_df)\n",
    "        if total_errors > 0:\n",
    "            if overstemming / total_errors > 0.6:\n",
    "                self.stat_weight = max(0.8, self.stat_weight - 0.05)\n",
    "                self.rule_weight = min(1.2, self.rule_weight + 0.05)\n",
    "            elif understemming / total_errors > 0.6:\n",
    "                self.rule_weight = max(0.8, self.rule_weight - 0.05)\n",
    "                self.stat_weight = min(1.2, self.stat_weight + 0.05)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Added {len(high_confidence_fixes)} high confidence fixes\")\n",
    "            print(f\"Added {len(medium_confidence_fixes)} medium confidence fixes\")\n",
    "            print(f\"Added {num_new} general exceptions\")\n",
    "            print(f\"Updated weights - Rule: {self.rule_weight:.2f}, Stat: {self.stat_weight:.2f}\")\n",
    "\n",
    "        systematic_fixes = self.find_systematic_errors(error_df)\n",
    "        for word, true_stem in systematic_fixes.items():\n",
    "            self.exception_dict[word] = true_stem\n",
    "        if self.verbose:\n",
    "            print(f\"Added {len(systematic_fixes)} systematic error fixes\")\n",
    "\n",
    "\n",
    "    def load_exceptions(self, exceptions_file):\n",
    "        if os.path.exists(exceptions_file):\n",
    "            try:\n",
    "                df = pd.read_excel(exceptions_file)\n",
    "                self.exception_dict = dict(zip(df['word'], df['stem']))\n",
    "                if self.verbose:\n",
    "                    print(f\"Loaded {len(self.exception_dict)} exceptions\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading exceptions: {e}\")\n",
    "                self.exception_dict = {}\n",
    "\n",
    "    def train_ngram_models(self, words, max_workers=4, sample_size=1000000):\n",
    "        print(\"Training n-gram models...\")\n",
    "        words = list(itertools.islice(words, sample_size))\n",
    "        char_data = \"\".join(words)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                'unigram': executor.submit(FreqDist, char_data),\n",
    "                'bigram': executor.submit(FreqDist, ngrams(char_data, 2)),\n",
    "                'trigram': executor.submit(FreqDist, ngrams(char_data, 3))\n",
    "            }\n",
    "            \n",
    "            for name, future in tqdm(futures.items(), desc=\"Training models\"):\n",
    "                self.ngram_models[name] = future.result()\n",
    "                \n",
    "        print(\"N-gram models trained successfully\")\n",
    "\n",
    "    def is_valid_stem(self, stem):\n",
    "        if not isinstance(stem, str) or len(stem) < self.min_stem_length:\n",
    "            return False\n",
    "            \n",
    "        if not stem:\n",
    "            return False\n",
    "            \n",
    "        valid_endings = ['ు', 'ి', 'ీ', 'ా', 'ె', 'ో', 'ూ', 'ే', 'అ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ',\n",
    "                        'క', 'గ', 'చ', 'జ', 'ట', 'డ', 'త', 'ద', 'న', 'ప', 'బ', 'మ', 'య', 'ర', 'ల', 'వ', 'శ', 'స', 'హ']\n",
    "        \n",
    "        last_char = stem[-1]\n",
    "        if last_char not in valid_endings:\n",
    "            return False\n",
    "            \n",
    "        if len(stem) >= 3 and 'trigram' in self.ngram_models:\n",
    "            trigram = tuple(stem[-3:])\n",
    "            if self.ngram_models['trigram'].get(trigram, 0) < 2:  \n",
    "                if len(stem) >= 2 and 'bigram' in self.ngram_models:\n",
    "                    bigram = tuple(stem[-2:])\n",
    "                    if self.ngram_models['bigram'].get(bigram, 0) < 3:  \n",
    "                        return False\n",
    "                        \n",
    "        consonant_pattern = re.compile(r'[క-హ][క-హ][క-హ][క-హ]+')  # More restrictive pattern\n",
    "        if consonant_pattern.search(stem):\n",
    "            return False  \n",
    "            \n",
    "        return True\n",
    "\n",
    "    def check_vowel_harmony(self, stem):\n",
    "        if len(stem) < 3:\n",
    "            return True\n",
    "            \n",
    "        vowels = [c for c in stem if c in 'ాిీుూెేైొోౌ']\n",
    "        if not vowels:\n",
    "            return True\n",
    "            \n",
    "        front_vowels = sum(1 for v in vowels if v in self.vowel_groups['front'])\n",
    "        back_vowels = sum(1 for v in vowels if v in self.vowel_groups['back'])\n",
    "        \n",
    "        if front_vowels > 2 and back_vowels == 0:\n",
    "            return True\n",
    "        if back_vowels > 2 and front_vowels == 0:\n",
    "            return True\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def rule_based_stem(self, word):\n",
    "        if not isinstance(word, str):\n",
    "            return word\n",
    "            \n",
    "        if word in self.exception_dict:\n",
    "            return self.exception_dict[word]\n",
    "            \n",
    "        for suffix in self.suffixes:\n",
    "            if word.endswith(suffix) and len(word) > len(suffix) + self.min_stem_length - 1:\n",
    "                candidate_stem = word[:-len(suffix)]\n",
    "                \n",
    "                if self.is_valid_stem(candidate_stem) and self.check_vowel_harmony(candidate_stem):\n",
    "                    return self.apply_morphological_rules(word, candidate_stem)\n",
    "                    \n",
    "        for pattern, replacement in self.sandhi_rules:\n",
    "            modified = re.sub(pattern, replacement, word)\n",
    "            if modified != word:\n",
    "                return self.rule_based_stem(modified) \n",
    "                \n",
    "        return word \n",
    "\n",
    "    def statistical_stem(self, word):\n",
    "        if not isinstance(word, str):\n",
    "            return word\n",
    "            \n",
    "        if word in self.exception_dict:\n",
    "            return self.exception_dict[word]\n",
    "            \n",
    "        best_stem, best_score = word, -1\n",
    "        max_suffix_len = min(self.max_suffix_length, len(word) - self.min_stem_length + 1)\n",
    "        \n",
    "        for suffix_len in range(1, max_suffix_len + 1):\n",
    "            candidate_stem = word[:-suffix_len]\n",
    "            \n",
    "            if not self.is_valid_stem(candidate_stem):\n",
    "                continue\n",
    "                \n",
    "            if len(candidate_stem) >= 3 and 'trigram' in self.ngram_models:\n",
    "                trigram_score = 0\n",
    "                for i in range(len(candidate_stem) - 2):\n",
    "                    trigram = tuple(candidate_stem[i:i+3])\n",
    "                    weight = (len(candidate_stem) - i) / len(candidate_stem)\n",
    "                    trigram_score += self.ngram_models['trigram'].get(trigram, 0) * weight\n",
    "            else:\n",
    "                trigram_score = 0\n",
    "                \n",
    "            if len(candidate_stem) >= 2 and 'bigram' in self.ngram_models:\n",
    "                bigram_score = 0\n",
    "                for i in range(len(candidate_stem) - 1):\n",
    "                    bigram = tuple(candidate_stem[i:i+2])\n",
    "                    weight = (len(candidate_stem) - i) / len(candidate_stem)\n",
    "                    bigram_score += self.ngram_models['bigram'].get(bigram, 0) * weight\n",
    "            else:\n",
    "                bigram_score = 0\n",
    "                \n",
    "            if 'unigram' in self.ngram_models:\n",
    "                unigram_score = sum(self.ngram_models['unigram'].get(c, 0) for c in candidate_stem) / len(candidate_stem) if candidate_stem else 0\n",
    "            else:\n",
    "                unigram_score = 0\n",
    "            \n",
    "            combined_score = (trigram_score * 3 + bigram_score * 2 + unigram_score) / 6\n",
    "            \n",
    "            length_penalty = 0.8 if len(candidate_stem) <= 2 else 1.0\n",
    "            \n",
    "            final_score = combined_score * length_penalty\n",
    "            \n",
    "            if final_score > best_score:\n",
    "                best_score, best_stem = final_score, candidate_stem\n",
    "                \n",
    "        return self.apply_morphological_rules(word, best_stem if best_score > 0 else word)\n",
    "\n",
    "    def similarity_score(self, word, stem1, stem2):\n",
    "        cache_key = f\"{word}|{stem1}|{stem2}\"\n",
    "        \n",
    "        if cache_key in self.similarity_cache:\n",
    "            return self.similarity_cache[cache_key]\n",
    "            \n",
    "        if not isinstance(stem1, str) or not isinstance(stem2, str):\n",
    "            return 0.0\n",
    "            \n",
    "        max_len = max(len(stem1), len(stem2))\n",
    "        if max_len == 0:\n",
    "            self.similarity_cache[cache_key] = 1.0\n",
    "            return 1.0\n",
    "            \n",
    "        edit_sim = 1 - (edit_distance(stem1, stem2) / max_len)\n",
    "        \n",
    "        chars1 = set(stem1)\n",
    "        chars2 = set(stem2)\n",
    "        if not chars1 or not chars2:\n",
    "            char_sim = 0.0\n",
    "        else:\n",
    "            char_sim = len(chars1.intersection(chars2)) / len(chars1.union(chars2))\n",
    "            \n",
    "        if len(stem1) >= 2 and len(stem2) >= 2:\n",
    "            bigrams1 = set(ngrams(stem1, 2))\n",
    "            bigrams2 = set(ngrams(stem2, 2))\n",
    "            ngram_sim = len(bigrams1.intersection(bigrams2)) / len(bigrams1.union(bigrams2)) if bigrams1 and bigrams2 else 0.0\n",
    "        else:\n",
    "            ngram_sim = 0.0\n",
    "            \n",
    "        final_score = (edit_sim * 0.5) + (char_sim * 0.3) + (ngram_sim * 0.2)\n",
    "        \n",
    "        self.similarity_cache[cache_key] = final_score\n",
    "        return final_score\n",
    "    \n",
    "    def detect_telugu_patterns(self, word):\n",
    "        \"\"\"Detect specific Telugu morphological patterns.\"\"\"\n",
    "        patterns = {\n",
    "            'verb_present': re.compile(r'తున్నా(ను|వు|డు|ము|రు|)$'),\n",
    "            'verb_past': re.compile(r'ా(ను|వు|డు|ము|రు|)$'),\n",
    "            'verb_future': re.compile(r'తా(ను|వు|డు|ము|రు|)$'),\n",
    "            'plural_noun': re.compile(r'(లు|ములు|వులు)$'),\n",
    "            'case_markers': re.compile(r'(లో|కి|తో|పై|నుండి)$')\n",
    "        }\n",
    "        \n",
    "        detected_patterns = {}\n",
    "        for pattern_name, regex in patterns.items():\n",
    "            if regex.search(word):\n",
    "                detected_patterns[pattern_name] = True\n",
    "        \n",
    "        return detected_patterns\n",
    "\n",
    "    def resolve_stem_disagreement(self, word, rule_stem, stat_stem):\n",
    "        \"\"\"Enhanced resolution for stem disagreements with linguistic insights.\"\"\"\n",
    "      \n",
    "        if word in self.exception_dict:\n",
    "            return self.exception_dict[word]\n",
    "        \n",
    "        if rule_stem in self.root_forms.values():\n",
    "            return rule_stem\n",
    "        if stat_stem in self.root_forms.values():\n",
    "            return stat_stem\n",
    "        \n",
    "        word_suffix = word[min(len(rule_stem), len(stat_stem)):]\n",
    "        if any(word_suffix.endswith(complex_suffix) for complex_suffix in ['తున్నాను', 'తున్నాడు', 'తున్నారు']):\n",
    "      \n",
    "            return rule_stem if len(rule_stem) >= self.min_stem_length else stat_stem\n",
    "        \n",
    "        if word.endswith(('లు', 'డు', 'ము', 'వు')):\n",
    "            return stat_stem if len(stat_stem) >= self.min_stem_length else rule_stem\n",
    "        \n",
    "        if rule_stem[-1] in 'ాిీుూెేైొోౌ' and not stat_stem[-1] in 'ాిీుూెేైొోౌ':\n",
    "            return rule_stem\n",
    "        if stat_stem[-1] in 'ాిీుూెేైొోౌ' and not rule_stem[-1] in 'ాిీుూెేైొోౌ':\n",
    "            return stat_stem\n",
    "        \n",
    "        if 'trigram' in self.ngram_models and len(rule_stem) >= 3 and len(stat_stem) >= 3:\n",
    "            rule_ngram_score = sum(self.ngram_models['trigram'].get(tuple(rule_stem[i:i+3]), 0) for i in range(len(rule_stem)-2))\n",
    "            stat_ngram_score = sum(self.ngram_models['trigram'].get(tuple(stat_stem[i:i+3]), 0) for i in range(len(stat_stem)-2))\n",
    "            \n",
    "            if rule_ngram_score > stat_ngram_score * 1.5:\n",
    "                return rule_stem\n",
    "            if stat_ngram_score > rule_ngram_score * 1.5:\n",
    "                return stat_stem\n",
    "        \n",
    "        rule_quality = self.compute_stem_quality(word, rule_stem) * self.rule_weight\n",
    "        stat_quality = self.compute_stem_quality(word, stat_stem) * self.stat_weight\n",
    "        \n",
    "        return rule_stem if rule_quality >= stat_quality else stat_stem\n",
    "\n",
    "    def hybrid_stem(self, word):\n",
    "        if not isinstance(word, str):\n",
    "            return word\n",
    "            \n",
    "        if word in self.exception_dict:\n",
    "            return self.exception_dict[word]\n",
    "            \n",
    "        rule_stem = self.rule_based_stem(word)\n",
    "        stat_stem = self.statistical_stem(word)\n",
    "        \n",
    "        if rule_stem == stat_stem:\n",
    "            return self.apply_morphological_rules(word, rule_stem)\n",
    "        \n",
    "        word_length = len(word)\n",
    "        has_complex_suffix = any(word.endswith(s) for s in [suffix for suffix in self.suffixes if len(suffix) > 3])\n",
    "        \n",
    "        rule_weight = self.rule_weight * (1.2 if has_complex_suffix else 1.0)\n",
    "        stat_weight = self.stat_weight * (1.2 if word_length > 8 else 1.0)\n",
    "        \n",
    "        if word_length <= 5:\n",
    "            rule_weight *= 1.3\n",
    "        \n",
    "        rule_quality = self.compute_stem_quality(word, rule_stem) * rule_weight\n",
    "        stat_quality = self.compute_stem_quality(word, stat_stem) * stat_weight\n",
    "        \n",
    "        if rule_quality > stat_quality * 1.3:\n",
    "            return self.apply_morphological_rules(word, rule_stem)\n",
    "        elif stat_quality > rule_quality * 1.3:\n",
    "            return self.apply_morphological_rules(word, stat_stem)\n",
    "        \n",
    "        patterns = self.detect_telugu_patterns(word)\n",
    "        if patterns.get('verb_present') or patterns.get('verb_past') or patterns.get('verb_future'):\n",
    "           \n",
    "            rule_weight *= 1.3\n",
    "        elif patterns.get('plural_noun') or patterns.get('case_markers'):\n",
    "            \n",
    "            stat_weight *= 1.2\n",
    "        \n",
    "        return self.resolve_stem_disagreement(word, rule_stem, stat_stem)\n",
    "    \n",
    "    def find_systematic_errors(self, error_df):\n",
    "        \"\"\"Find patterns in errors to automatically generate new exceptions.\"\"\"\n",
    "        error_patterns = defaultdict(list)\n",
    "        \n",
    "        for _, row in error_df.iterrows():\n",
    "            word = row['word']\n",
    "            true_stem = row['true_stem']\n",
    "            pred_stem = row['predicted_stem']\n",
    "            \n",
    "            for i in range(1, min(6, len(word))):\n",
    "                ending = word[-i:]\n",
    "                error_patterns[ending].append((word, true_stem, pred_stem))\n",
    "        \n",
    "        systematic_errors = {}\n",
    "        for ending, errors in error_patterns.items():\n",
    "            if len(errors) >= 3:  \n",
    "                pred_true_pairs = [(e[2], e[1]) for e in errors]\n",
    "                if len(set(pred_true_pairs)) <= 2: \n",
    "                    for word, true_stem, _ in errors:\n",
    "                        systematic_errors[word] = true_stem\n",
    "        \n",
    "        return systematic_errors\n",
    "        \n",
    "    def compute_stem_quality(self, word, stem):\n",
    "        \"\"\"Compute a quality score for a proposed stem.\"\"\"\n",
    "        if not isinstance(stem, str) or not stem:\n",
    "            return 0.0\n",
    "            \n",
    "        morph_score = 1.0 if self.is_valid_stem(stem) else 0.4\n",
    "        \n",
    "        ngram_score = 0.0\n",
    "        if len(stem) >= 3 and 'trigram' in self.ngram_models:\n",
    "            trigrams = list(ngrams(stem, 3))\n",
    "            weighted_sum = 0\n",
    "            for i, trigram in enumerate(trigrams):\n",
    "                position_weight = 1.0 + (i / (len(trigrams) or 1)) * 0.7\n",
    "                weighted_sum += self.ngram_models['trigram'].get(trigram, 0) * position_weight\n",
    "            ngram_score = min(weighted_sum / (10 * (len(trigrams) or 1)), 1.0)\n",
    "        \n",
    "        if len(stem) < 3:\n",
    "            length_ratio = 0.5 * (len(stem) / len(word))\n",
    "        else:\n",
    "            length_ratio = min(len(stem) / len(word), 0.8)\n",
    "        \n",
    "        struct_score = 1.0 if self.check_vowel_harmony(stem) else 0.6\n",
    "        \n",
    "        root_bonus = 1.3 if stem in self.root_forms.values() else 1.0\n",
    "        \n",
    "        ending_bonus = 1.0\n",
    "        if stem.endswith(('ు', 'ి', 'ా', 'క', 'గ', 'చ', 'జ', 'ట', 'డ', 'త', 'ద', 'న', 'ప', 'బ', 'మ', 'య', 'ర', 'ల', 'వ', 'స')):\n",
    "            ending_bonus = 1.2\n",
    "         \n",
    "        quality_score = (morph_score * 0.35 + \n",
    "                        ngram_score * 0.25 + \n",
    "                        length_ratio * 0.15 + \n",
    "                        struct_score * 0.15) * root_bonus * ending_bonus\n",
    "                        \n",
    "        return quality_score\n",
    "\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        prediction_methods = {\n",
    "            'rule_based': self.rule_based_stem,\n",
    "            'statistical': self.statistical_stem,\n",
    "            'hybrid': self.hybrid_stem\n",
    "        }\n",
    "        \n",
    "        predictions = {method: [] for method in prediction_methods}\n",
    "        for word, true_stem in tqdm(test_data, desc=\"Evaluating\"):\n",
    "            for method, predictor in prediction_methods.items():\n",
    "                predictions[method].append(predictor(word))\n",
    "        \n",
    "        y_true = [true_stem for _, true_stem in test_data]\n",
    "        \n",
    "        accuracies = {method: np.mean([pred == true for pred, true in zip(preds, y_true)]) * 100 \n",
    "                      for method, preds in predictions.items()}\n",
    "        \n",
    "        reports = {}\n",
    "        error_analysis = {}\n",
    "        \n",
    "        for method, preds in predictions.items():\n",
    "            try:\n",
    "                reports[method] = pd.DataFrame(classification_report(y_true, preds, output_dict=True, zero_division=0)).T\n",
    "                \n",
    "                errors = [(word, pred, true) for (word, _), pred, true in zip(test_data, preds, y_true) if pred != true]\n",
    "                error_by_length = defaultdict(int)\n",
    "                for word, _, _ in errors:\n",
    "                    error_by_length[len(word)] += 1\n",
    "                \n",
    "                error_analysis[method] = {\n",
    "                    'total_errors': len(errors),\n",
    "                    'error_rate': len(errors) / len(test_data) * 100,\n",
    "                    'errors_by_length': dict(error_by_length),\n",
    "                    'avg_error_word_length': np.mean([len(word) for word, _, _ in errors]) if errors else 0\n",
    "                }\n",
    "                \n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Could not generate report for {method} due to: {e}\")\n",
    "                reports[method] = None\n",
    "                error_analysis[method] = {'error': str(e)}\n",
    "        \n",
    "        return accuracies, reports, error_analysis\n",
    "\n",
    "    def tune_parameters(self, validation_data):\n",
    "        print(\"Tuning stemmer parameters...\")\n",
    "        best_accuracy = 0\n",
    "        best_hybrid_advantage = 0\n",
    "        best_params = {\n",
    "            'rule_weight': self.rule_weight,\n",
    "            'stat_weight': self.stat_weight,\n",
    "            'min_stem_length': self.min_stem_length,\n",
    "            'max_suffix_length': self.max_suffix_length\n",
    "        }\n",
    "        \n",
    "        param_grid = {\n",
    "        'rule_weight': [0.75, 0.85, 0.95, 1.0, 1.05, 1.15, 1.25],\n",
    "        'stat_weight': [0.75, 0.85, 0.95, 1.0, 1.05, 1.15, 1.25],\n",
    "        'min_stem_length': [2, 3],\n",
    "        'max_suffix_length': [5, 6, 7, 8]\n",
    "        }\n",
    "        \n",
    "        sample_size = min(500, len(validation_data))\n",
    "        sampled_validation = validation_data[:sample_size]\n",
    "        \n",
    "        for rule_w in param_grid['rule_weight']:\n",
    "            for stat_w in param_grid['stat_weight']:\n",
    "                for min_len in param_grid['min_stem_length']:\n",
    "                    for max_suffix in param_grid['max_suffix_length']:\n",
    "                        self.rule_weight = rule_w\n",
    "                        self.stat_weight = stat_w\n",
    "                        self.min_stem_length = min_len\n",
    "                        self.max_suffix_length = max_suffix\n",
    "                        \n",
    "                        hybrid_correct = 0\n",
    "                        rule_correct = 0\n",
    "                        stat_correct = 0\n",
    "                        \n",
    "                        for word, true_stem in sampled_validation:\n",
    "                            if self.hybrid_stem(word) == true_stem:\n",
    "                                hybrid_correct += 1\n",
    "                            if self.rule_based_stem(word) == true_stem:\n",
    "                                rule_correct += 1\n",
    "                            if self.statistical_stem(word) == true_stem:\n",
    "                                stat_correct += 1\n",
    "                        \n",
    "                        hybrid_accuracy = hybrid_correct / sample_size * 100\n",
    "                        rule_accuracy = rule_correct / sample_size * 100\n",
    "                        stat_accuracy = stat_correct / sample_size * 100\n",
    "                        \n",
    "                        best_individual = max(rule_accuracy, stat_accuracy)\n",
    "                        hybrid_advantage = hybrid_accuracy - best_individual\n",
    "                        \n",
    "                        if hybrid_accuracy > best_accuracy:\n",
    "                            best_accuracy = hybrid_accuracy\n",
    "                            best_hybrid_advantage = hybrid_advantage\n",
    "                            best_params = {\n",
    "                                'rule_weight': rule_w,\n",
    "                                'stat_weight': stat_w,\n",
    "                                'min_stem_length': min_len,\n",
    "                                'max_suffix_length': max_suffix\n",
    "                            }\n",
    "        \n",
    "        self.rule_weight = best_params['rule_weight']\n",
    "        self.stat_weight = best_params['stat_weight']\n",
    "        self.min_stem_length = best_params['min_stem_length']\n",
    "        self.max_suffix_length = best_params['max_suffix_length']\n",
    "        \n",
    "        print(f\"Best parameters found: {best_params} with accuracy: {best_accuracy:.2f}% (advantage: +{best_hybrid_advantage:.2f}%)\")\n",
    "        return best_params, best_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def load_datasets(base_dir):\n",
    "    try:\n",
    "        annotated_df = pd.read_excel(os.path.join(base_dir, \"training_data.xlsx\"))\n",
    "        test_df = pd.read_excel(os.path.join(base_dir, \"test_set.xlsx\"))\n",
    "        \n",
    "        corpus_file = os.path.join(base_dir, \"processed_telugu_corpus.txt\")\n",
    "        if os.path.exists(corpus_file):\n",
    "            with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "                words = (word.strip() for word in f.read().split())\n",
    "        else:\n",
    "            words = annotated_df['word'].tolist()\n",
    "        \n",
    "        print(f\"Loaded {len(annotated_df)} training samples, {len(test_df)} test samples\")\n",
    "        return words, annotated_df, test_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading datasets: {e}\")\n",
    "        return [], pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "def split_data(data_df, test_size=0.2):\n",
    "    indices = np.random.permutation(len(data_df))\n",
    "    test_count = int(len(data_df) * test_size)\n",
    "    test_indices = indices[:test_count]\n",
    "    train_indices = indices[test_count:]\n",
    "    \n",
    "    train_df = data_df.iloc[train_indices]\n",
    "    val_df = data_df.iloc[test_indices]\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    stemmer = TeluguStemmer()\n",
    "    print(\"Initializing Telugu Stemmer...\")\n",
    "    \n",
    "    words, annotated_df, test_df = load_datasets(BASE_DIR)\n",
    "    \n",
    "    train_df, val_df = split_data(annotated_df, test_size=0.15)\n",
    "    \n",
    "    test_data = list(zip(test_df['word'], test_df['stem']))\n",
    "    train_words, train_stems = train_df['word'].tolist(), train_df['stem'].tolist()\n",
    "    val_words, val_stems = val_df['word'].tolist(), val_df['stem'].tolist()\n",
    "    \n",
    "    training_data = list(zip(train_words, train_stems))\n",
    "    validation_data = list(zip(val_words, val_stems))\n",
    "    \n",
    "    stemmer.training_data = training_data\n",
    "\n",
    "    print(\"\\nInitial training...\")\n",
    "    stemmer.load_exceptions(os.path.join(BASE_DIR, \"telugu_exceptions.xlsx\"))\n",
    "    stemmer.train_ngram_models(words, sample_size=800000) \n",
    "    \n",
    "    print(\"\\nParameter tuning...\")\n",
    "    stemmer.tune_parameters(validation_data[:800])  \n",
    "    \n",
    "    word_endings = Counter()\n",
    "    for word, _ in training_data:\n",
    "        for i in range(1, min(6, len(word))):\n",
    "            word_endings[word[-i:]] += 1\n",
    "    \n",
    "    common_suffixes = [suffix for suffix, count in word_endings.most_common(50) \n",
    "                      if count > 5 and suffix not in stemmer.suffixes]\n",
    "    \n",
    "    stemmer.suffixes.extend(common_suffixes)\n",
    "    stemmer.suffixes = sorted(stemmer.suffixes, key=len, reverse=True)\n",
    "\n",
    "    print(\"\\nInitial evaluation...\")\n",
    "    accuracies, reports, error_analysis = stemmer.evaluate(test_data)\n",
    "    \n",
    "    print(\"\\nInitial Accuracy Scores:\")\n",
    "    for method, acc in accuracies.items():\n",
    "        print(f\"{method.title()}: {acc:.2f}%\")\n",
    "        print(f\"  Errors: {error_analysis[method]['total_errors']}, Rate: {error_analysis[method]['error_rate']:.2f}%\")\n",
    "\n",
    "    print(\"\\nPerforming iterative improvement...\")\n",
    "    max_iterations = 5  \n",
    "    prev_hybrid_accuracy = accuracies['hybrid']\n",
    "    improvement_threshold = 0.05  \n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"\\nIteration {iteration + 1}\")\n",
    "        \n",
    "        hybrid_errors = stemmer.analyze_errors(test_data)\n",
    "        \n",
    "        rule_errors = []\n",
    "        for word, true_stem in test_data:\n",
    "            if stemmer.rule_based_stem(word) != true_stem:\n",
    "                rule_errors.append((word, true_stem))\n",
    "        \n",
    "        stat_errors = []\n",
    "        for word, true_stem in test_data:\n",
    "            if stemmer.statistical_stem(word) != true_stem:\n",
    "                stat_errors.append((word, true_stem))\n",
    "        \n",
    "        hybrid_error_tuples = [(row['word'], row['true_stem']) for _, row in hybrid_errors.iterrows()]\n",
    "        \n",
    "        all_errors = hybrid_error_tuples + rule_errors[:20] + stat_errors[:20]\n",
    "        \n",
    "        seen = set()\n",
    "        unique_errors = []\n",
    "        for item in all_errors:\n",
    "            if item not in seen:\n",
    "                seen.add(item)\n",
    "                unique_errors.append(item)\n",
    "        \n",
    "        error_words = [word for word, _ in unique_errors]\n",
    "        error_true_stems = [true_stem for _, true_stem in unique_errors]\n",
    "        temp_data = list(zip(error_words, error_true_stems))\n",
    "        error_df = stemmer.analyze_errors(temp_data)\n",
    "        \n",
    "        current_error_rate = len(hybrid_errors)/len(test_data)\n",
    "        print(f\"Found {len(hybrid_errors)} hybrid errors, {len(rule_errors)} rule errors, {len(stat_errors)} statistical errors\")\n",
    "        print(f\"Combined unique errors: {len(error_df)}\")\n",
    "        print(f\"Current hybrid error rate: {current_error_rate*100:.2f}%\")\n",
    "        \n",
    "        if len(hybrid_errors) == 0 or (iteration > 0 and accuracies['hybrid'] - prev_hybrid_accuracy < improvement_threshold):\n",
    "            print(\"No significant improvement, stopping iterations.\")\n",
    "            break\n",
    "            \n",
    "        prev_hybrid_accuracy = accuracies['hybrid']\n",
    "        \n",
    "        stemmer.update_from_errors(error_df.sample(min(75, len(error_df))))\n",
    "        \n",
    "        if iteration % 2 == 1:\n",
    "            stemmer.tune_parameters(validation_data[:500])\n",
    "        \n",
    "        new_accuracies, new_reports, new_error_analysis = stemmer.evaluate(test_data)\n",
    "        \n",
    "        improvement = {k: new_accuracies[k] - accuracies[k] for k in accuracies}\n",
    "        \n",
    "        print(\"\\nAccuracy Improvements:\")\n",
    "        for method, imp in improvement.items():\n",
    "            print(f\"{method.title()}: {imp:+.2f}%\")\n",
    "            \n",
    "        accuracies, reports, error_analysis = new_accuracies, new_reports, new_error_analysis\n",
    "    \n",
    "    print(\"\\nFinal Accuracy Scores:\")\n",
    "    for method, acc in accuracies.items():\n",
    "        print(f\"{method.title()}: {acc:.2f}%\")\n",
    "        print(f\"  Errors: {error_analysis[method]['total_errors']}, Rate: {error_analysis[method]['error_rate']:.2f}%\")\n",
    "    \n",
    "   \n",
    "    \n",
    "    model_file = os.path.join(BASE_DIR, \"telugu_stemmer_model.pkl\")\n",
    "    try:\n",
    "        with open(model_file, 'wb') as f:\n",
    "            pickle.dump(stemmer, f)\n",
    "        print(f\"\\nModel saved to {model_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    # NEW CODE: Print some example predictions\n",
    "    print(\"\\n--- Sample Stemming Results ---\")\n",
    "    \n",
    "    # Use some words from test data as examples\n",
    "    if test_data:\n",
    "        print(\"\\nTest Data Examples:\")\n",
    "        for i, (word, true_stem) in enumerate(test_data[:10]):  # Print first 10 examples\n",
    "            predicted_stem = stemmer.hybrid_stem(word)\n",
    "            print(f\"Word: {word}\")\n",
    "            print(f\"True stem: {true_stem}\")\n",
    "            print(f\"Predicted stem: {predicted_stem}\")\n",
    "            print(f\"Match: {'✓' if predicted_stem == true_stem else '✗'}\")\n",
    "            print(\"-\" * 30)\n",
    "    \n",
    "    # You can also add some custom examples if you have specific words to test\n",
    "    custom_examples = [\n",
    "        \"రాజులు\", \"చేస్తున్నాను\", \"పిల్లలకు\", \"పుస్తకములు\", \"వచ్చినది\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nCustom Examples:\")\n",
    "    for word in custom_examples:\n",
    "        rule_stem = stemmer.rule_based_stem(word)\n",
    "        stat_stem = stemmer.statistical_stem(word)\n",
    "        hybrid_stem = stemmer.hybrid_stem(word)\n",
    "        \n",
    "        print(f\"Word: {word}\")\n",
    "        print(f\"Rule-based stem: {rule_stem}\")\n",
    "        print(f\"Statistical stem: {stat_stem}\")\n",
    "        print(f\"Hybrid stem: {hybrid_stem}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    # Print words with different results between methods\n",
    "    print(\"\\nWords with different stemming results:\")\n",
    "    divergent_results = []\n",
    "    for word, true_stem in test_data:\n",
    "        rule_stem = stemmer.rule_based_stem(word)\n",
    "        stat_stem = stemmer.statistical_stem(word)\n",
    "        hybrid_stem = stemmer.hybrid_stem(word)\n",
    "        \n",
    "        if rule_stem != stat_stem or rule_stem != hybrid_stem or stat_stem != hybrid_stem:\n",
    "            divergent_results.append((word, true_stem, rule_stem, stat_stem, hybrid_stem))\n",
    "    \n",
    "    for i, (word, true_stem, rule_stem, stat_stem, hybrid_stem) in enumerate(divergent_results[:5]):\n",
    "        print(f\"Word: {word}\")\n",
    "        print(f\"True stem: {true_stem}\")\n",
    "        print(f\"Rule-based: {rule_stem}\")\n",
    "        print(f\"Statistical: {stat_stem}\")\n",
    "        print(f\"Hybrid: {hybrid_stem}\")\n",
    "        def get_best_method(rule_stem, stat_stem, hybrid_stem, true_stem):\n",
    "            stems = {'rule': rule_stem, 'statistical': stat_stem, 'hybrid': hybrid_stem}\n",
    "            return max(['Rule', 'Statistical', 'Hybrid'], \n",
    "                    key=lambda m: 1 if stems[m.lower()] == true_stem else 0)\n",
    "\n",
    "        print(f\"Best method: {get_best_method(rule_stem, stat_stem, hybrid_stem, true_stem)}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41da724-27eb-4ee7-910b-974ec322ce40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
